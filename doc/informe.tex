\documentclass[12pt,letterpaper, margin = 3cm]{article}
\footskip = 50pt
\topmargin = -3cm
\usepackage[spanish]{babel}
%\usepackage[ansinew]{inputenc}
\usepackage[utf8]{inputenc}
% \usepackage[latin1]{inputenc}
\usepackage[letterpaper,includeheadfoot, top=0.5cm, bottom=5.0cm, right=2.0cm, left=2.0cm]{geometry}
\renewcommand{\familydefault}{\sfdefault}

\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{url}
%\usepackage{pdfpages}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{subfig}

\usepackage{listings} %Codigo
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{mauve},
  commentstyle=\color{blue},
  stringstyle=\color{dkgreen},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

\begin{document}
%\begin{sf}
% --------------- ---------PORTADA --------------------------------------------
\newpage
\pagestyle{fancy}
\fancyhf{}
%-------------------- CABECERA ---------------------
\fancyhead[L]{ \includegraphics[scale=0.09]{img/logodcc.png} }
%------------------ TÍTULO -----------------------
\vspace*{5cm}
\begin{center}
\huge  {Tarea 2}\\
\Huge {KD-Trees}\\
\vspace{6cm}
\end{center}
%----------------- NOMBRES ------------------------
\vfill
\begin{flushright}
\begin{tabular}{ll}
Autores: & Claudio Berroeta\\
& Sebastián Ferrada \\
Profesor: & Pablo Barceló\\
Auxiliar: & Miguel Romero\\
Ayudantes: & Javiera Born\\
& Giselle Font\\
& \today\\
\end{tabular}
\end{flushright}

% ·············· ENCABEZADO - PIE DE PAGINA ············
\newpage
\pagestyle{fancy}
\fancyhf{}

%Encabezado
%\fancyhead[L]{\rightmark}
\fancyhead[L]{\small \rm \textit{Sección \rightmark}} %Izquierda
\fancyhead[R]{\small \rm \textbf{\thepage}} %Derecha


\fancyfoot[C]{\small \rm \textit{KD-Trees\\}} %Izquierda
%\fancyfoot[R]{\small \rm \textit{Pie de página - Derecha}} %Derecha
%\fancyfoot[C]{\thepage} %Centro

\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}
\newcommand{\fancyfootnotetext}[2]{%
  \fancypagestyle{dingens}{%
    \fancyfoot[LO,RE]{\parbox{12cm}{\footnotemark[#1]\footnotesize #2}}%
  }%
  \thispagestyle{dingens}%
}

% =============== INDICE ===============

\tableofcontents
%\listoffigures

% =============== SECCION ===============
\newpage
\section{Introducción}

% ----- Texto Introducción------
En el presente informe se pretende mostrar un análisis sobre la performance de la construcción y consultas a Árboles KD, los cuales alojan puntos del plano y los organizan de forma tal que se pueda consultar fácilmente por el vecino más cercano a un punto dado.
Este problema de vecino más cercano es particularmente útil, por ejemplo, si se tiene una cadena de locales de pizza, saber desde qué local despachar la pizza, dada una dirección de pedido.\\
La idea es, dado un conjunto de puntos, particionarlos según una recta  $x=c$. El valor de la constante de la recta, $c$, puede ser la media aritmética o la mediana entre las coordenadas $x$ de los puntos. Particionamos el conjunto de puntos según si están a la derecha o a la izquerda de la recta y proseguimos recursivamente particionando los dos conjuntos obtenidos, pero esta vez, particionando respecto a una recta $y=c'$.\\
Se estudiarán los tiempos de construcción, altura del árbol construído, espacio utilizado por el árbol y el tiempo que toma una consulta sobre el árbol en 4 situaciones concretas, dadas por las combinaciones entre árbol que particiona por mediana o árbol que particiona por media y puntos aleatorios o puntos de baja discrepancia.
% ----------------------------------

\subsection{Hipótesis}
\begin{enumerate}
\item Se espera que la altura de los árboles sea $O(\log(n))$, pues cada árbol intentará particionar el conjunto de puntos en dos mitades con la misma cantidad de elementos cada una.
\item Dado lo anterior, se espera que la construcción tome tiempo $O(n\log(n))$, pues recorremos los $n$ puntos en los $\log(n)$. niveles que se espera tener.
\item Dada la topología de la estructura, se espera que las consultas de vecino más cercano, se ejecuten en tiempo $O(\log(n))$.
\item Además, se espera que el espacio utilizado por el árbol no sea superior a $O(n)$.
\item Dado que tanto la media, como la mediana son medidas de centralización casi igualmente certeras, no se espera que la elección de uno por sobre el otro cause diferencias importantes en cuanto a la eficiencia de la construcción del árbol o del balanceo de la estructura resultante, lo que implica que tampoco habría una optimización en el tiempo de consulta ni en la altura del árbol ni en el espacio utilizado.
\item El tener puntos de baja discrepancia, se espera que nos permita descartar ramas completas del árbol en el segundo recorrido en la consulta de punto más cercano, pues hay una menor probabilidad de intersección/colisión respecto a una distribución aleatoria de puntos, por lo que se espera que las consultas en árboles construidos sobre puntos de baja discrepancia sean más rápidas.
\item Claramente, los experimentos en memoria secundaria demorarán mayor tiempo en ejecutarse, con $O(n)$ accesos a disco, en promedio durante la construcción del árbol ($n$ escrituras) y cantidad logarítmica de accesos para la búsqueda (Asumiendo un nodo por bloque, sin buffer).
\end{enumerate}

\subsection{Ambiente Operacional}
Los experimentos fueron ejecutados en un Notebook con Sistema Operativo Windows 8 de 64 bits, 8 Gb de
memoria RAM y procesador intel Core i7 de 3.630 GHz. Para la implementación se utilizó el Java Development Kit v1.7.

\newpage
\section{Diseño de la Estructura y Experimentos}
El diseño con el que se implementó la estructura, es bastante clásico, un árbol que contiene una raíz, un nodo, y una serie de nodos que guardan punteros a sus hijos y un valor, el cual puede ser una recta divisoria o el valor de un punto, propiamente tal.

\subsection{Estructura del Árbol}
Un árbol tiene un nodo raíz, sabe como construirse a partir de un set de puntos y es capaz de recibir consultas de vecino más cercano a un punto. Este árbol debe ``decidir'' cómo particionar los nodos, es por esto que se crearon dos tipos de árboles, un árbol que particiona por mediana y otro por media aritmética. Entonces usando un Template Pattern, el árbol KD genérico sabe cómo construirse y son los hijos los que definen el criterio para seleccionar la recta de partición, según el siguiente diagrama de clases:

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.85]{img/TreeStructure.png}}
 \caption{UML para las clases que implementan la estructura}
\end{figure}

Por otro lado, son Nodos quienes guardan la información del árbol, pues este solo tiene un puntero al nodo raíz. Hay nodos de dos tipos, los internos guardan las rectas que particionan el espacio y las hojas guardan la información de los puntos, por lo que se tiene el siguente diagrama de clases:

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.7]{img/nodeStructure.png}}
 \caption{UML para las clases que implementan los nodos}
\end{figure}
\newpage
Los nodos también son los responsables de recorrer el árbol durante las consultas, hacia los hijos y hacia el padre, además son los que calculan la intersección entre los sectores que delimitan y el círculo de consulta.

\subsection{Construcción del Árbol}

% -----Explicación de cómo funciona la construcción-------
La construcción se hace recursivamente, con caso base un conjunto de puntos de tamaño $1$. En cada fase se particiona el conjunto de acuerdo al criterio de mediana o media, intercalando el eje en cada nivel. El código es bastante simple y se presenta a continuación:
\begin{lstlisting}
public KDNode constructKdtree(List<KDPoint> points, Axis axis){
        if(points.size() == 1 ){
            return new KDLeaf(points.get(0));
        }
        KDLine line = getLine(points, axis) ; //This line depends on the Tree citeria
        List<List<KDPoint>> partition = makePartition(points,line);
        return new KDInternalNode(line,
                                  constructKdtree(partition.get(0),axis.negated()),
                                  constructKdtree(partition.get(1),axis.negated()));
    } 
\end{lstlisting}

Las diferentes implementaciones de getLine y el método partition pueden encontrarse en los anexos
% -------------------------------------------------------------

\subsection{Consultas de Vecino más cercano}

% -----Explicación de cómo funcionan las consultas------
Las consultas buscan descendentemente desde la raíz, el nodo del cuadrante que incluye al punto de consultas. Recordemos que el árbol crea una suerte de rejilla sobre el espacio, entonces la primera búsqueda le indica cual es el cuadrante del punto consultado y se calcula la distancia entre este y el punto del árbol de ese cuadrante.\\
Con esta distancia volvemos a subir por el árbol y vemos si el cuadrante que enmarca el otro hijo del nodo intersecta el circulo con centro el punto de consulta y radio la distancia que tenemos guardada. Así vamos subiendo y si encontramos alguna distancia mejor, la guardamos. Este comportamiento se registra en el siguiente código:
\newpage
\begin{lstlisting}
public KDPoint closestNeighbor(KDPoint q){
    KDLeaf currentBest = root.searchNeighbor(q);//looks for node, same sector than q
    double currentDistance = currentBest.distance(q);
    KDNode actual = currentBest.getParent();
    KDNode prev = currentBest;  
    
    while(actual!= null){
        KDLeaf temp = actual.anotherSearch(prev, currentDistance,q); //looks for a best fit
        if(temp.distance(q)<currentDistance){
            currentBest = temp;
            currentDistance = currentBest.distance(q);
        }
        prev = actual;
        actual = actual.getParent();
   }
   return currentBest.getPoint();
}
\end{lstlisting}

Los métodos searchNeighbot y anotherSearch se pueden encontrar en los anexos
% -------------------------------------------------------------
\newpage
\subsection{Diseño de los Experimentos}
Los experimentos consistieron en construir árboles KD con arreglos de puntos de tamaños sucesivamente más grandes (tamaño $2^i$, $i\in\{10,...,20\}$ en las diferentes combinaciones de criterio de partición y de distribución de puntos y medimos su tiempo, la altura del árbol y la cantidad de espacio en memoria que utiliza. Luego a cada sabor de árbol se le realizan consultas y se mide el tiempo que demora en contestar. Cada experimento se repite las veces necesarias para que el error sea menor al 5\% (i.e., hasta que $\frac{stdv}{mean}\leq 0.05$). Para detalles de la implementación de la batería de experimentos, referirse al código fuente adjunto.

\newpage
\section{Resultados}
Los diferentes criterios de partición y tipo de distribución de puntos nos generaban 4 instancias de análisis para cada parámetro a medir.

\subsection{Tiempo de construcción}
El experimento consistía en cuánto demoraba en ser contruído el árbol. Los resultados se pueden visualizar en el siguiente gráfico:

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.5]{img/ConstructTime.png}}
 \caption{Tiempo de construcción de los KDTrees}
\end{figure}

 \begin{figure}[ht!] \centering
 \subfloat[Mean]{\includegraphics[scale=0.3]{img//ConstructTime1.png}}
 \subfloat[Median]{\includegraphics[scale=0.3]{img//ConstructTime2.png}}
 \caption{Tiempo de construcción}\label{holaquetal}
 \end{figure}

A simple vista podemos confundir el orden de construcción, pero analizando por separado podemos decir que el tiempo de construcción es del orden de $O(n log(n))$.


\newpage
\subsection{Altura}
El experimento consistía en calcular la altura que alcanzaban los KDTrees de un tamaño determinado. Los resultados se pueden visualizar en el siguiente gráfico:

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.7]{img/height.png}}
 \caption{Altura de los KDTrees}
\end{figure}

Claramente podemos apreciar que las alturas de los árboles son del orden de $O(log(n))$, destacando que los árboles con puntos distribuidos de manera uniforme tienen una altura mas baja en comparación a los árboles de distribución aleatoria.
\newpage
\subsection{Tiempo de consulta}
El experimento consistía en medir cuánto demoraban en encontrar los vecinos más cercanos de una serie de puntos. Los resultados se pueden visualizar en el siguiente gráfico:

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.6]{img/QueryTime.png}}
 \caption{Tiempo de consulta de los KDTrees}
\end{figure}

Se puede apreciar que el tiempo de consulta es del orden $O(log(n))$, también se puede visualizar que los puntos de baja discrepancia demoran un poco menos en comparación a los puntos aleatorios.

\newpage
\subsection{ Espacio de un KDTree} 
El experimento consistía en saber cuánto espacio ocupa cada KDTree, se usaron valores constantes en bytes para cada nodo. 

\begin{figure}[ht!]
 \centering
\fbox{\includegraphics[scale=0.6]{img/Space.png}}
 \caption{Espacio ocupado por los KDTrees}
\end{figure}

Podemos ver que el tamaño de un KDTree es del orden $O(n)$ y además que para cada tipo de KDTree el tamaño ocupado es el mismo, esto se debe a que se insertan la misma cantidad de nodos y no existen estructuras externas.



\newpage
\section{Conclusiones}
\begin{enumerate} 
\item Efectivamente la altura de los árboles es del orden $O(log(n))$ y además los KDTrees creados a partir de puntos de baja discrepancia poseen una menor altura, es decir la construcción es un poco más balanceada.
\item La construcción de KDTrees tomó tiempo $O(n log(n))$, tal como fue previsto. Aunque los tiempo entre $mean$ y $median$ es muy distancia, lo cual lo atribuimos al $overheight$ del método median.
\item También se comprobó que las consultas de vecino más cercano se ejecutaron en tiempo O(log(n)) para todo tipo de construcción y sin importar la distribución de puntos.
\item Se verificó que el espacio ocupado por el árbol sea del orden de $O(n)$. Todos los árboles de la misma cantidad de puntos ocupan el mismo espacio.
\item Nuestra hipótesis sobre las diferencias sobre la construcción del árbol fue errónea, dado que en la construcción del árbol el $mean$ fue mucho menor al otro.
\item Que los puntos estén distribuidos de una manera uniforme presentó leves mejoras en cuanto a los tiempos de medición, tal como se predijo en la hipótesis.
\end{enumerate}




% ============= ANEXOS =====================
\newpage
\section{Anexos}




% ============= FIN DE DOCUMENTO ==============
\end{document}

% % ················ IMAGEN ·················
% \begin{figure}[ht!]
% \centering
% \fbox{\includegraphics[scale=0.6]{img/torneo.png}}
% \caption{Torneo}\label{torneo}
% \end{figure}
% %··········································

% % ················ IMAGEN DOBLE ·················
% \begin{figure}[ht!] \centering
% \subfloat[Hola]{\includegraphics[scale=0.44]{img/holaquetal.png}}
% \subfloat[Que tal]{\includegraphics[scale=0.45]{img/holaquetal1.png}}
% \caption{Holaquetal}\label{holaquetal}
% \end{figure}
% %··········································